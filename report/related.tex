\section{Related Works}
\label{sec:related}
The usage of speculative execution for reducing user-perceived latency has a
significant history of research and study. 
Mickens, {\it et al}. propos in system Crom that 
explores the role of pre-fetching and speculative execution within the web
application and distributed systems domain~\cite{crom}. Crom allows for a
more-generalized pre-computation framework for the traditionally non-speculative
javascript event handlers. By running a shadow clone of the user's browsing
session and speculating on the shadow, if the user selects a speculated-upon
browser context, then their system presents the precomputed result to the user's
real web browser. Their results were promising, they showed that the background
speculation overhead ``easily fits within user think time'', and that  speculative
significant reduced user-perceived latencies (in this case from 3,427 ms to 399
ms).

Patterson, and {\it et al}. study the use of speculative prefetching and caching
to hide disk IO time~\cite{diskio}.  The authors create a system that asks
applications to disclose their access patterns and uses this information to
batch I/O, exploit I/O parallelism, and to dynamically prefetch data from the
disk to main memory. The system specialized system had promising results with an
up to 36 percent speed up on application run-time and with an average speed of
13 percent speed-up across all the tested applications. 
Korner investigates the usage of intelligent file caching for a
remote distributed file service to reduce and hide the significant network
latencies inherent in the remote file system~\cite{korner}.
Along the same lines on hiding network latencies, Davison, and {\it et al}. explore
the current benefits of web cache prefetching, the broad issues and side effects
that plague the current approach, and proposes suggestions to alleviate these
issues and side effects~\cite{davison}.

The paper~\cite{davison} states that traditional file system and memory system
caching frameworks don't apply well to the web caching domain because of a few
key unique attributes of the domain space. Web objects have a variable and
unknown cache-ability which complicates the determination of which objects to
pre-cache, web servers are vulnerable to over-commitment in the case of too many
users pre-fetching a significant amount of web objects, and the usage of GET for
object retrieval is inherently flawed because of the numerous side effects for
the content owner and the intended content recipients.

Pitkow, and {\it et al}. propose a system on the extraction of a prediction
model for user web-surfing paths from a large amount of historical user-traces
through their system~\cite{pitkow}. The authors demonstrate that K-th order
Markov Models and N-grams can both efficiently store and represent user's paths.
The results showed that a model prediction accuracy can be significantly
improved by storing longer path dependencies at the cost of increased storage
space. The paper also explores the usage of longest repeating sequences, or
LRS, to reduce the complexity and storage space needed for stored paths.

Furthermore, Padmanabhan, and {\it et al}. propose a way to dynamically examine
the server-collected statistics amount typical client access and
requests to create prefetching behavior hints for the server's respective
clients~\cite{padmanabhan1996using}.  The results show that their user trace-driven
predictions for prefetching significantly reduces the average access time for
both high-bandwidth and low-bandwidth clients. They noted that the improvement
in web object access time also incurred the cost of increased network traffic.
Thus another piece of evidence for the argument that generalized execution
should allow developers to easily exchange excess resources (network, disk,
memory) for a benefit towards file request and access-response time.

Recent system called {\it Nectar}~\cite{nectar} carries on this motivation for a
higher-level view of computation and speculative `pre-work'. In the paper, the
authors design and implement a system that more intelligently manages data and
computation within the data-center computing environment. Nectar uses the
unification of the concepts of data and computation by tagging and associating
data with its respective computation. This allows for the automated management
of data access, computation budget, and the caching service that is shared
across the datacenter.
The key insight that Gunda et al. had was to abstract the idea of computation
from the various data that can be pre-executed (or pre-fetched or pre-cached).
This novel lens to view the relationship between computation and data within the
data center, allowed them to more easily and more intelligently examine
potential pre-computations and caching opportunities for common computations
that can be computed only once and then be reused by others.

At the University of Michigan, researchers Benjamin Wester, Peter M.Chen, and Jason Flinn evaluated the cost and benefits of extending support for application-specific speculation into the underlying operating system. In the 2011 paper, they divide the concept of speculative execution into two equal parts: a policy that specifies which operations and values to preemptively compute, and the underlying mechanisms that support speculation such as check-pointing, rollback, and the tracking of causalities. This work represents another step forward into the idea of de-coupling speculative execution from the exact details and minutiae of the specific applications. Their system performed well, for example they hid up to 85 percent of the program load time by predicting the program's launch, and increased SSL connection latency in the FireFox web-browser by 15 percent. Their more application-agnostic system that resides in the operating system significantly speed up user-perceived latencies for a "modest performance trade-off" and it executed only 8 percent slower than a hand-written customized speculative execution implementation for each specific application.

Alspaugh's, Chen's, Lin's, Ganapathi's, Hearst's, and Katz's 2014 paper investigated the improvement of the Splunk data analysis tools based on the significantly large amount of user-trace logs (over 200,000) from the data analytics platform Splunk. Alspaugh's et al.'s analysis found that most Splunk users' actions in their system consisted of filtering, reformatting, and summarizing data. They also noticed that less-skilled users were more dependent on data from logs to propel their decision making. One of the clever ways the authors capture Splunk users' paths through the program is through state machine diagrams with links of weighted probabilities for transitions between states. This research explored the specialized UI tune-ups and customized features that can be implemented to help with their population's common use-cases, but doesn't entertain the idea of generalized speculative execution that is application-agnostic.

The 'Time Warp' operating system from University of California: Los Angeles, reduced the user-perceived latency for distributed simulations by speculating and pre-computing probable simulation queries across the remote-nodes. This system was specialized for the distributed system architecture of their simulations, so they made many domain specific assumptions such as distributed processes being unable to utilize heap storage and remote file caching. A common theme across these prior works is to specially tailor and tightly couple the implementation of the speculative execution to the application that is being speculated upon. 

Jayachandran, Tunga, Kamat, and Nandi's DICE system that used speculative execution and distributed aggregation to enable smoother and more responsive interactive data cube exploration. In their 2014 paper, the authors realized that many users care more about interactive response-times than exact data accuracy, and their system implements this trade-off by selectively sampling the data. Like previous works, the system relies on the assumption that there is a 'thinking' or in this case 'perusing' phase after the user receives results and opportunistically uses this extra time and extra resources to execute and preemptively cache the results of the most probable future queries, based on the previous queries. The implemented DICE had very promising results, with every single test-user noticing and preferring the speed-up version of the application. Their application saved about 7 seconds for each 54 second session. Throughout all of these previous works, their systems either customized and specifically tailored the speculation system with the targeted applications, or only generalized in a limited manner to speed-up perceived latency for common use-cases of their application. Our proposed Pythia system seeks to generalize in a more abstract fashion to decouple the implementation of broader parts of speculative execution from the minute details of each speculating program.
